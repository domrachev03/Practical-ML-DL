{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Practical machine learning and deep learning. Lab 3\n\n# Deep Learning in Natural Language Processing\n\n# [Competition](https://www.kaggle.com/t/4677b08c063f433ba1eb8f3543af90b4)\n\n## Goal\n\nYour goal is to implement Neural Network to classify Amazon Products reviews. \n\n## Submission\n\nSubmission format is described at competition page.","metadata":{}},{"cell_type":"markdown","source":"## Data preprocessing\n\nData preprocessing is an essential step in building a Machine Learning model and depending on how well the data has been preprocessed.\n\nIn NLP, text preprocessing is the first step in the process of building a model.\n\nThe various text preprocessing steps are:\n\n* Tokenization\n* Lower casing\n* Stop words removal\n* Stemming\n* Lemmatization\n\nThese various text preprocessing steps are widely used for dimensionality reduction.\n\nFirst, let's read the input data and then perform preprocessing steps","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_dataframe = pd.read_csv('/kaggle/input/pmldl-week-3-dl-in-natural-language-processing/train.csv')\ntest_dataframe = pd.read_csv('/kaggle/input/pmldl-week-3-dl-in-natural-language-processing/test.csv')\n\ntrain_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:42.456729Z","iopub.execute_input":"2023-09-18T14:48:42.457233Z","iopub.status.idle":"2023-09-18T14:48:42.883665Z","shell.execute_reply.started":"2023-09-18T14:48:42.457200Z","shell.execute_reply":"2023-09-18T14:48:42.882503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the training data we have `4` features (`Title`, `Helpfulness`, `Score` and `Text`) with target category (`Category`). For the test features are the same, except for target column.\n\nFirst, let's write functions for preprocessing helpfulness and score feature in case we needed them.","metadata":{}},{"cell_type":"code","source":"def preprocess_score_inplace(df):\n    \"\"\"\n    Normalizes score to make it from 0 to 1.\n    \n    For now it is from 1.0 to 5.0, so natural choice\n    is to normalize by (f - 1.0)/4.0\n    \"\"\"\n    df['Score'] = (df['Score'] - df['Score'].min()) / (df['Score'].max() - df['Score'].min())\n    return df\n\ndef preprocess_helpfulness_inplace(df):\n    \"\"\"\n    Splits feature by '/' and normalize helpfulness to make it from 0 to 1\n    \n    The total number of assessments can be 0, so let's substitute it\n    with 1. The resulting helpfulness still will be zero but we\n    remove the possibility of division by zero exception.\n    \"\"\"\n    _splitted = df['Helpfulness'].str.split('/', expand=True)\n    _helpful, _total = _splitted[0], _splitted[1]\n    _total.replace(\"0\", \"1\", inplace=True)\n    df['Helpfulness'] = _helpful.astype(int) / _total.astype(int)\n    return df    ","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:42.885999Z","iopub.execute_input":"2023-09-18T14:48:42.886755Z","iopub.status.idle":"2023-09-18T14:48:42.897995Z","shell.execute_reply.started":"2023-09-18T14:48:42.886713Z","shell.execute_reply":"2023-09-18T14:48:42.896652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The two other features are both text. For simplicity, let's remove concatenate them so that we will have one full text feature. The resulting code is also a function.","metadata":{}},{"cell_type":"code","source":"def concat_title_text_inplace(df):\n    \"\"\"\n    Concatenates Title and Text columns together\n    \"\"\"\n    df['Text'] = df['Title'] + \" \" + df['Text']\n    df.drop('Title', axis=1, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:43.009056Z","iopub.execute_input":"2023-09-18T14:48:43.009454Z","iopub.status.idle":"2023-09-18T14:48:43.016223Z","shell.execute_reply.started":"2023-09-18T14:48:43.009420Z","shell.execute_reply":"2023-09-18T14:48:43.015215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, encode the target categories, so that the output is become an index","metadata":{}},{"cell_type":"code","source":"enc = LabelEncoder()\n\ncat_encoded = enc.fit_transform(train_dataframe['Category'].values.reshape(-1, 1)).astype(np.int16)\ntrain_df_enc = train_dataframe.copy()\ntrain_df_enc['Category'] = cat_encoded\n\ntrain_df_enc.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:43.259249Z","iopub.execute_input":"2023-09-18T14:48:43.259666Z","iopub.status.idle":"2023-09-18T14:48:43.306040Z","shell.execute_reply.started":"2023-09-18T14:48:43.259633Z","shell.execute_reply":"2023-09-18T14:48:43.304589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize our first stage of preprocessing.","metadata":{}},{"cell_type":"code","source":"train_copy = train_df_enc.head().copy()\n\npreprocess_score_inplace(\n    preprocess_helpfulness_inplace(\n        concat_title_text_inplace(train_copy)\n    )\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:43.544648Z","iopub.execute_input":"2023-09-18T14:48:43.545006Z","iopub.status.idle":"2023-09-18T14:48:43.573279Z","shell.execute_reply.started":"2023-09-18T14:48:43.544976Z","shell.execute_reply":"2023-09-18T14:48:43.572240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text cleaning\n\nFor text cleaning, you can use lower casting, punctuation removal, numbers removal, tokenization, stop words removal, stemming. This will get a perfectly cleaned text without any garbage information.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef lower_text(text: str):\n    return text.lower()\n\ndef remove_numbers(text: str):\n    \"\"\"\n    Substitute all punctuations with space in case of\n    \"there is5dogs\".\n    \n    If subs with '' -> \"there isdogs\"\n    With ' ' -> there is dogs\n    \"\"\"\n    text_nonum = re.sub(r'\\d+', ' ', text)\n    return text_nonum\n\ndef remove_punctuation(text: str):\n    \"\"\"\n    Substitute all punctiations with space in case of\n    \"hello!nice to meet you\"\n    \n    If subs with '' -> \"hellonice to meet you\"\n    With ' ' -> \"hello nice to meet you\"\n    \"\"\"\n    text_nopunct = re.sub(r'[^a-z|\\s]+', ' ', text)\n    return text_nopunct\n\ndef remove_multiple_spaces(text: str):\n    text_no_doublespace = re.sub('\\s+', ' ', text).strip()\n    return text_no_doublespace","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:44.005127Z","iopub.execute_input":"2023-09-18T14:48:44.005480Z","iopub.status.idle":"2023-09-18T14:48:44.012701Z","shell.execute_reply.started":"2023-09-18T14:48:44.005450Z","shell.execute_reply":"2023-09-18T14:48:44.011457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This will give us clean text.","metadata":{}},{"cell_type":"code","source":"sample_text = train_copy['Text'][4]\n\n_lowered = lower_text(sample_text)\n_without_numbers = remove_numbers(_lowered)\n_without_punct = remove_punctuation(_without_numbers)\n_single_spaced = remove_multiple_spaces(_without_punct)\n\nprint(sample_text)\nprint('-'*10)\nprint(_lowered)\nprint('-'*10)\nprint(_without_numbers)\nprint('-'*10)\nprint(_without_punct)\nprint('-'*10)\nprint(_single_spaced)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:44.652679Z","iopub.execute_input":"2023-09-18T14:48:44.653366Z","iopub.status.idle":"2023-09-18T14:48:44.661453Z","shell.execute_reply.started":"2023-09-18T14:48:44.653326Z","shell.execute_reply":"2023-09-18T14:48:44.660175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, harder preprocessing: tokenization, stop words removal and stemming.\nFor that you can use several packages, but we encourage you to use `nltk` - Natural Language ToolKit as well as `torchtext`.\n\n\nTake a look at:\n* `nltk.tokenize.word_tokenize` or `torchtext.data.utils.get_tokenizer` for tokenization\n* `nltk.corpus.stopwords` for stop words removal\n* `nltk.stem.PorterStemmer` for stemming","metadata":{}},{"cell_type":"code","source":"import nltk\nimport nltk.tokenize as tkn\nimport nltk.corpus as corpus\nimport nltk.stem as stem\n\ndef tokenize_text(text: str) -> list[str]:\n    return tkn.word_tokenize(text)\n\ndef remove_stop_words(tokenized_text: list[str]) -> list[str]:\n    stop_words = set(corpus.stopwords.words('english'))\n    cleaned_text = [w for w in tokenized_text if w not in stop_words]\n    return cleaned_text\n\ndef stem_words(tokenized_text: list[str]) -> list[str]:\n    stemmer = stem.PorterStemmer()\n    return [stemmer.stem(word) for word in tokenized_text]","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:45.413560Z","iopub.execute_input":"2023-09-18T14:48:45.414011Z","iopub.status.idle":"2023-09-18T14:48:46.000606Z","shell.execute_reply.started":"2023-09-18T14:48:45.413975Z","shell.execute_reply":"2023-09-18T14:48:45.999648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_tokenized = tokenize_text(_single_spaced)\n_without_sw = remove_stop_words(_tokenized)\n_stemmed = stem_words(_without_sw)\n\nprint(_single_spaced)\nprint('-'*10)\nprint(_tokenized)\nprint('-'*10)\nprint(_without_sw)\nprint('-'*10)\nprint(_stemmed)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:46.002468Z","iopub.execute_input":"2023-09-18T14:48:46.002896Z","iopub.status.idle":"2023-09-18T14:48:46.029193Z","shell.execute_reply.started":"2023-09-18T14:48:46.002862Z","shell.execute_reply":"2023-09-18T14:48:46.028336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, there is a lot of words removed as well as the unnecessary language rules (I mean stems, com'on). Now we are able to construct full cleaning preprocessing stage.","metadata":{}},{"cell_type":"code","source":"def preprocessing_stage(text):\n    _lowered = lower_text(text)\n    _without_numbers = remove_numbers(_lowered)\n    _without_punct = remove_punctuation(_without_numbers)\n    _single_spaced = remove_multiple_spaces(_without_punct)\n    _tokenized = tokenize_text(_single_spaced)\n    _without_sw = remove_stop_words(_tokenized)\n    _stemmed = stem_words(_without_sw)\n    \n    return _stemmed\n\ndef clean_text_inplace(df):\n    df['Text'] = df['Text'].apply(preprocessing_stage)\n    return df\n\ndef preprocess(df):\n    df.fillna(\" \", inplace=True)\n    _preprocess_score = preprocess_score_inplace(df)\n    _preprocess_helpfulness = preprocess_helpfulness_inplace(_preprocess_score)\n    _concatted = concat_title_text_inplace(_preprocess_helpfulness)\n\n    _cleaned = clean_text_inplace(_concatted)\n    \n    return _cleaned\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:47.369237Z","iopub.execute_input":"2023-09-18T14:48:47.369710Z","iopub.status.idle":"2023-09-18T14:48:47.388153Z","shell.execute_reply.started":"2023-09-18T14:48:47.369668Z","shell.execute_reply":"2023-09-18T14:48:47.387070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now let's apply it on our train and test dataframes.","metadata":{}},{"cell_type":"code","source":"train_preprocessed = preprocess(train_df_enc)\ntest_preprocessed = preprocess(test_dataframe)\n\ntrain_preprocessed.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:48:48.811309Z","iopub.execute_input":"2023-09-18T14:48:48.811976Z","iopub.status.idle":"2023-09-18T14:50:53.572308Z","shell.execute_reply.started":"2023-09-18T14:48:48.811942Z","shell.execute_reply":"2023-09-18T14:50:53.571043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's split our original train dataset into train and val sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nratio = 0.1\ntrain, val = train_test_split(\n    train_preprocessed, stratify=train_preprocessed['Category'], test_size=ratio, random_state=420\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:50:53.574348Z","iopub.execute_input":"2023-09-18T14:50:53.574724Z","iopub.status.idle":"2023-09-18T14:50:53.605850Z","shell.execute_reply.started":"2023-09-18T14:50:53.574690Z","shell.execute_reply":"2023-09-18T14:50:53.604979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now, for the best result, lets get rid of pandas so that nothing is stopping us from working with torchtext. For that let's create an iterator that is going to yield samples for us.","metadata":{}},{"cell_type":"markdown","source":"# Creating dataloaders\n\nFirst, you should generate our vocab from the train set.\n\nFor that, use `torchtext.vocab.build_vocab_from_iterator`.","metadata":{}},{"cell_type":"code","source":"from torchtext.vocab import build_vocab_from_iterator\n\ndef yield_tokens(df):\n    for _, sample in train.iterrows():\n        yield sample.to_list()[2]\n\n\n# Define special symbols and indices\nUNK_IDX, PAD_IDX = 0, 1\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['<unk>', '<pad>']\n\nvocab = build_vocab_from_iterator(yield_tokens(train['Text']), specials=special_symbols)\nvocab.set_default_index(UNK_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:50:53.608632Z","iopub.execute_input":"2023-09-18T14:50:53.609019Z","iopub.status.idle":"2023-09-18T14:50:59.240272Z","shell.execute_reply.started":"2023-09-18T14:50:53.608959Z","shell.execute_reply":"2023-09-18T14:50:59.239228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And then use our vocab to encode the tokenized sequence","metadata":{}},{"cell_type":"code","source":"sample = train['Text'][2]\nprint(sample)\nencoded = vocab(sample)\nprint(encoded)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T14:50:59.243955Z","iopub.execute_input":"2023-09-18T14:50:59.244548Z","iopub.status.idle":"2023-09-18T14:50:59.251402Z","shell.execute_reply.started":"2023-09-18T14:50:59.244511Z","shell.execute_reply":"2023-09-18T14:50:59.250434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can define our collate function and create dataloaders","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\ntorch.manual_seed(420)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# From https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\ndef collate_batch(batch):\n    label_list, text_list, offsets = [], [], [0]\n    for _, _, _text, _label in batch:\n        label_list.append(_label)\n        _preprocessed = torch.tensor(vocab(_text), dtype=torch.int64)\n        offsets.append(len(_preprocessed))\n        text_list.append(_preprocessed)\n        \n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    text_list = torch.cat(text_list)\n    return label_list.to(device), text_list.to(device), offsets.to(device)\n    \ntrain_dataloader = DataLoader(\n    train.to_numpy(), batch_size=128, shuffle=True, collate_fn=collate_batch\n)\n\nval_dataloader = DataLoader(\n    val.to_numpy(), batch_size=128, shuffle=False, collate_fn=collate_batch\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:12.696790Z","iopub.execute_input":"2023-09-18T15:23:12.697692Z","iopub.status.idle":"2023-09-18T15:23:12.717986Z","shell.execute_reply.started":"2023-09-18T15:23:12.697660Z","shell.execute_reply":"2023-09-18T15:23:12.717042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Network\n\n\nFor writing a network you can use `torch.nn.Embedding` or `torch.nn.EmbeddingBag`. This will allow your netorwk to learn embedding vector for your tokens.\n\nAs for the other modules in your network, consider these options:\n* Simple Linear layers, activations, basic stuff that goes into the network\n* There is a possible of not using the offsets (indices of sequences) in the formard, put use predefined sequence length (maximum length, some value, etc.). If this is an option for you, change the `collate_batch` function according to your architecture.\n* You could use all this recurrent stuff (RNN, GRU, LSTM, even Transformer, all up to you), but remembder about the dimentions and hidden states\n* If you have any quiestions - google it","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim \nimport torchmetrics","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:13.470563Z","iopub.execute_input":"2023-09-18T15:23:13.470930Z","iopub.status.idle":"2023-09-18T15:23:13.476082Z","shell.execute_reply.started":"2023-09-18T15:23:13.470901Z","shell.execute_reply":"2023-09-18T15:23:13.475156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From https://github.com/Bjarten/early-stopping-pytorch\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print            \n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:13.625479Z","iopub.execute_input":"2023-09-18T15:23:13.625746Z","iopub.status.idle":"2023-09-18T15:23:13.636058Z","shell.execute_reply.started":"2023-09-18T15:23:13.625722Z","shell.execute_reply":"2023-09-18T15:23:13.634978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextClassificationModel(nn.Module):\n    def __init__(self, num_classes):\n        super(TextClassificationModel, self).__init__()\n        \n        self.n_classes = num_classes\n        self.embed = nn.EmbeddingBag(len(vocab), 128, sparse=False)\n        self.fc1 = nn.Linear(128, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 256)\n        self.fc4 = nn.Linear(256, num_classes)\n\n    def forward(self, text, offsets):\n        text = self.embed(text, offsets)\n        text = F.relu(self.fc1(text))\n        text = F.relu(self.fc2(text))\n        text = F.relu(self.fc3(text))\n#         text = F.relu(self.fc4(text))\n#         text = F.relu(self.fc5(text))\n        text = self.fc4(text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:13.754386Z","iopub.execute_input":"2023-09-18T15:23:13.754681Z","iopub.status.idle":"2023-09-18T15:23:13.762538Z","shell.execute_reply.started":"2023-09-18T15:23:13.754632Z","shell.execute_reply":"2023-09-18T15:23:13.761531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.autonotebook import tqdm\n\ndef train_one_epoch(\n    model,\n    loader,\n    optimizer,\n    loss_fn,\n    sсheduler,\n    f1_score,\n    epoch_num=-1\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch_num}: train\",\n        leave=True,\n    )\n    model.train()\n    train_loss = 0.0\n    for i, (labels, texts, offsets) in loop:\n        # zero the parameter gradients\n        model.zero_grad()\n\n        # forward pass\n        outputs = model(texts, offsets)\n        # loss calculation\n        loss = loss_fn(outputs, labels)\n        \n        # backward pass\n        loss.backward()\n\n        # optimizer run\n        optimizer.step()\n        \n        train_loss += loss.item()\n        loop.set_postfix({\"loss\": train_loss/(i * len(labels))})\n    \n    # sheduler step        \n    sсheduler.step(train_loss)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:13.870486Z","iopub.execute_input":"2023-09-18T15:23:13.870790Z","iopub.status.idle":"2023-09-18T15:23:13.878696Z","shell.execute_reply.started":"2023-09-18T15:23:13.870762Z","shell.execute_reply":"2023-09-18T15:23:13.877742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_one_epoch(\n    model,\n    loader,\n    loss_fn,\n    epoch_num=-1,\n    f1_score=None, \n    best_so_far=0.0,\n    ckpt_path='best.pt'\n):\n    \n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch_num}: val\",\n        leave=True,\n    )\n    val_loss = 0.0\n    score = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, (labels, texts, offsets) in loop:\n            labels = labels.to(device)\n    \n            # forward pass\n            outputs = model(texts, offsets)\n            # loss calculation\n            loss = loss_fn(outputs, labels)\n            predicted = torch.argmax(outputs.data, dim=1).to(device)\n            total += predicted.size(0)\n            correct += (predicted == labels).sum()\n\n            val_loss += loss.item()\n            score += f1_score(predicted, labels)\n            loop.set_postfix({\"loss\": val_loss/total, \"acc\": correct / total, \"f1\": f1_score(predicted, labels)})\n        \n        score /= len(loader)\n        print(f\"F1 score: {score}\")\n        if score > best_so_far:\n            torch.save(model.state_dict(), 'best_model.pt')\n            best_so_far = f1_score\n\n    return best_so_far, val_loss","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:47.023884Z","iopub.execute_input":"2023-09-18T15:23:47.024317Z","iopub.status.idle":"2023-09-18T15:23:47.038190Z","shell.execute_reply.started":"2023-09-18T15:23:47.024287Z","shell.execute_reply":"2023-09-18T15:23:47.036899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100\nmodel = TextClassificationModel(6).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=1, verbose=True)\nstopper = EarlyStopping(delta=1e-3)\nf1_score = torchmetrics.F1Score(task = \"multiclass\", num_classes = 6).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:47.123552Z","iopub.execute_input":"2023-09-18T15:23:47.123922Z","iopub.status.idle":"2023-09-18T15:23:47.174411Z","shell.execute_reply.started":"2023-09-18T15:23:47.123892Z","shell.execute_reply":"2023-09-18T15:23:47.173474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best = -float('inf')\nfor epoch in range(epochs):\n    train_one_epoch(model, train_dataloader, optimizer, loss_fn, scheduler, f1_score, epoch_num=epoch)\n    best, val_loss = val_one_epoch(model, val_dataloader, loss_fn, epoch, f1_score, best_so_far=best)\n    \n    stopper(val_loss, model)\n    \n    if stopper.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:47.243870Z","iopub.execute_input":"2023-09-18T15:23:47.244369Z","iopub.status.idle":"2023-09-18T15:24:23.264940Z","shell.execute_reply.started":"2023-09-18T15:23:47.244330Z","shell.execute_reply":"2023-09-18T15:24:23.263897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"def collate_batch(batch):\n    text_list, offsets = [], [0]\n    for _, _, _, _text in batch:\n        _preprocessed = torch.tensor(vocab(_text), dtype=torch.int64)\n        offsets.append(len(_preprocessed))\n        text_list.append(_preprocessed)\n        \n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    text_list = torch.cat(text_list)\n    return text_list.to(device), offsets.to(device)\n\ntest_dataloader = DataLoader(\n    test_preprocessed.to_numpy(), batch_size=128, shuffle=False, collate_fn=collate_batch\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:24:57.234925Z","iopub.execute_input":"2023-09-18T15:24:57.235310Z","iopub.status.idle":"2023-09-18T15:24:57.246063Z","shell.execute_reply.started":"2023-09-18T15:24:57.235242Z","shell.execute_reply":"2023-09-18T15:24:57.244484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(\n    model,\n    loader,\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=\"Predictions:\",\n        leave=True,\n    )\n    predictions = []\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, (texts, offsets) in loop:\n            \n            # forward pass and loss calculation\n            outputs = model(texts, offsets)\n            \n            _, predicted = torch.max(outputs.data, 1)\n            predictions += predicted.detach().cpu().tolist()\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:27:45.213684Z","iopub.execute_input":"2023-09-18T15:27:45.214541Z","iopub.status.idle":"2023-09-18T15:27:45.222440Z","shell.execute_reply.started":"2023-09-18T15:27:45.214507Z","shell.execute_reply":"2023-09-18T15:27:45.221478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt = torch.load(\"/kaggle/working/best_model.pt\")\nmodel.load_state_dict(ckpt)\n\npredictions = predict(model, test_dataloader)\npredictions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:27:45.542849Z","iopub.execute_input":"2023-09-18T15:27:45.543365Z","iopub.status.idle":"2023-09-18T15:27:45.928181Z","shell.execute_reply.started":"2023-09-18T15:27:45.543326Z","shell.execute_reply":"2023-09-18T15:27:45.927126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.Series(enc.inverse_transform(predictions))\nresults.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:27:46.654249Z","iopub.execute_input":"2023-09-18T15:27:46.655107Z","iopub.status.idle":"2023-09-18T15:27:46.670134Z","shell.execute_reply.started":"2023-09-18T15:27:46.655065Z","shell.execute_reply":"2023-09-18T15:27:46.669025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.to_csv('submission.csv', index_label='id')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:27:55.262894Z","iopub.execute_input":"2023-09-18T15:27:55.263549Z","iopub.status.idle":"2023-09-18T15:27:55.297140Z","shell.execute_reply.started":"2023-09-18T15:27:55.263517Z","shell.execute_reply":"2023-09-18T15:27:55.296243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}