{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# Practical machine learning and deep learning. Lab 4\n\n# Many-to-many NLP task.\n\n# [Competition](https://www.kaggle.com/t/afa89356762e438cad5f04bf0e23f3ce)\n\n## Goal\n\nYour goal is to implement Neural Network for tagging the part-of-speech entities.\n\n## Submission\n\nSubmission format is described at competition page.\n\n> Remember, you can use any structure of the solution. The template classes/function in this file is just the tip for you. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:28.763875Z","iopub.execute_input":"2023-09-24T12:53:28.764398Z","iopub.status.idle":"2023-09-24T12:53:28.771837Z","shell.execute_reply.started":"2023-09-24T12:53:28.764354Z","shell.execute_reply":"2023-09-24T12:53:28.770652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data reading and preprocessing","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/pmldl-week4-many-to-many-nlp-task/train.csv')\ntest = pd.read_csv('/kaggle/input/pmldl-week4-many-to-many-nlp-task/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:29.033280Z","iopub.execute_input":"2023-09-24T12:53:29.033660Z","iopub.status.idle":"2023-09-24T12:53:29.716551Z","shell.execute_reply.started":"2023-09-24T12:53:29.033630Z","shell.execute_reply":"2023-09-24T12:53:29.715492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:29.718928Z","iopub.execute_input":"2023-09-24T12:53:29.719329Z","iopub.status.idle":"2023-09-24T12:53:29.731167Z","shell.execute_reply.started":"2023-09-24T12:53:29.719283Z","shell.execute_reply":"2023-09-24T12:53:29.730018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.isna().any(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:58:32.639109Z","iopub.execute_input":"2023-09-24T12:58:32.639513Z","iopub.status.idle":"2023-09-24T12:58:32.913698Z","shell.execute_reply.started":"2023-09-24T12:58:32.639475Z","shell.execute_reply":"2023-09-24T12:58:32.912641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:29.733103Z","iopub.execute_input":"2023-09-24T12:53:29.733562Z","iopub.status.idle":"2023-09-24T12:53:29.749559Z","shell.execute_reply.started":"2023-09-24T12:53:29.733522Z","shell.execute_reply":"2023-09-24T12:53:29.748659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, let's divide dataset on train and validation. And split the dataframe according to random split.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nVALIDATION_RATIO = 0.1\ntrain_split, val_split = train_test_split(range(train['sentence_id'].max()), test_size=VALIDATION_RATIO, random_state=420)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:29.890055Z","iopub.execute_input":"2023-09-24T12:53:29.890415Z","iopub.status.idle":"2023-09-24T12:53:29.915058Z","shell.execute_reply.started":"2023-09-24T12:53:29.890386Z","shell.execute_reply":"2023-09-24T12:53:29.913957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And then split the original dataframe by ids that we splitted.","metadata":{}},{"cell_type":"code","source":"train_dataframe = train[train['sentence_id'].isin(train_split)]\nval_dataframe = train[train['sentence_id'].isin(val_split)]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:30.516208Z","iopub.execute_input":"2023-09-24T12:53:30.516587Z","iopub.status.idle":"2023-09-24T12:53:30.600713Z","shell.execute_reply.started":"2023-09-24T12:53:30.516549Z","shell.execute_reply":"2023-09-24T12:53:30.599660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_tags = train['tag'].unique().tolist()\ncat2idx = {tag: i for i, tag in enumerate(pos_tags)}\nidx2cat = {v: k for k, v in cat2idx.items()}\n\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:30.882744Z","iopub.execute_input":"2023-09-24T12:53:30.883154Z","iopub.status.idle":"2023-09-24T12:53:30.982089Z","shell.execute_reply.started":"2023-09-24T12:53:30.883121Z","shell.execute_reply":"2023-09-24T12:53:30.981129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check that given train data is valid: ","metadata":{}},{"cell_type":"markdown","source":"## Generating a dataset","metadata":{}},{"cell_type":"markdown","source":"### Analysis of the sentence length","metadata":{}},{"cell_type":"code","source":"print(train[\"entity_id\"].max())\nprint(test[\"entity_id\"].max())","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:32.382104Z","iopub.execute_input":"2023-09-24T12:53:32.382484Z","iopub.status.idle":"2023-09-24T12:53:32.390251Z","shell.execute_reply.started":"2023-09-24T12:53:32.382452Z","shell.execute_reply":"2023-09-24T12:53:32.389022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len((train[train[\"entity_id\"] > 128][\"sentence_id\"]).unique()))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:32.756802Z","iopub.execute_input":"2023-09-24T12:53:32.757199Z","iopub.status.idle":"2023-09-24T12:53:32.768927Z","shell.execute_reply.started":"2023-09-24T12:53:32.757164Z","shell.execute_reply":"2023-09-24T12:53:32.767729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.hist(train[\"entity_id\"], log=True, bins=27)\nplt.savefig(\"pic.png\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:33.261046Z","iopub.execute_input":"2023-09-24T12:53:33.261415Z","iopub.status.idle":"2023-09-24T12:53:34.023221Z","shell.execute_reply.started":"2023-09-24T12:53:33.261382Z","shell.execute_reply":"2023-09-24T12:53:34.022078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on that, the maximal length of 128 seems to be optimal","metadata":{}},{"cell_type":"code","source":"max_words_in_sentense = 128","metadata":{"execution":{"iopub.status.busy":"2023-09-24T12:53:34.071201Z","iopub.execute_input":"2023-09-24T12:53:34.071500Z","iopub.status.idle":"2023-09-24T12:53:34.076301Z","shell.execute_reply.started":"2023-09-24T12:53:34.071473Z","shell.execute_reply":"2023-09-24T12:53:34.075243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating a dataset","metadata":{}},{"cell_type":"markdown","source":"For working with datasets more efficiently, let's create separate classes for datasets. \n\n","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.manual_seed(420)\nfrom torchtext.vocab import build_vocab_from_iterator\n\n\nclass PosTaggingDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe: pd.DataFrame, cat2idx, vocab=None, max_size=100, if_train=True):\n        self.dataframe = dataframe\n        self.cat2idx = cat2idx\n        self.if_train = if_train\n        self._preprocess()\n        self.vocab = vocab or self._generate_vocab()\n\n    def _preprocess(self):\n        # Fill missing tag to `other` - `X`  \n        self.dataframe[\"entity\"].fillna(\"<unk>\", inplace=True)\n        \n        # Clean entities column\n        self.dataframe = self.dataframe.drop(columns=\"entity_id\")\n        \n        # Split the dataset, so that we will have \n        # full sentences and full tags by the same index\n        aug_dataframe = self.dataframe.copy()\n        aug_dataframe[\"entity\"] += \" \"\n        if self.if_train:\n            aug_dataframe[\"tag\"] += \" \"\n        sentences = aug_dataframe.groupby([\"sentence_id\"]).sum()\n        \n        self.sentences = sentences[\"entity\"].apply(lambda x: x.strip().split(\" \"))\n        if self.if_train:\n            self.tags = sentences[\"tag\"].apply(lambda x: x.strip().split(\" \"))\n\n    def _generate_vocab(self):\n        vocab = build_vocab_from_iterator(self.sentences, specials=special_symbols)\n        vocab.set_default_index(UNK_IDX)\n            \n        return vocab\n        \n    def _get_sentence(self, index: int) -> list:\n        # retrieves sentence from dataset by index\n        sent = self.sentences.iat[index]\n        \n        return self.vocab(sent)\n\n    def _get_labels(self, index: int) -> list:\n        # retrieves tags from dataset by index\n        tag = self.tags.iat[index]\n        return [self.cat2idx[tag_i] for tag_i in tag]\n\n    def __getitem__(self, index) -> tuple[list, list] | list:\n        if self.if_train:\n            return self._get_sentence(index), self._get_labels(index)\n        else:\n            return self._get_sentence(index)\n    def __len__(self) -> int:\n        return len(self.sentences)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:10:49.420446Z","iopub.execute_input":"2023-09-24T14:10:49.420914Z","iopub.status.idle":"2023-09-24T14:10:49.435953Z","shell.execute_reply.started":"2023-09-24T14:10:49.420877Z","shell.execute_reply":"2023-09-24T14:10:49.434759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create train dataset\ntrain_dataset = PosTaggingDataset(train_dataframe, cat2idx)\ntrain_vocab = train_dataset.vocab\nval_dataset = PosTaggingDataset(val_dataframe, cat2idx, train_vocab)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:30:38.887105Z","iopub.execute_input":"2023-09-24T13:30:38.889023Z","iopub.status.idle":"2023-09-24T13:30:40.991665Z","shell.execute_reply.started":"2023-09-24T13:30:38.888973Z","shell.execute_reply":"2023-09-24T13:30:40.990712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now we are able to create dataloader faster, because we created torch datasets","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef collate_batch(batch: list):\n    # Collate list of samples into tensor batch\n    # As an input we have list of pair from dataset:\n    # [([ent1, ent2, ...], [tag1, tag2, ...]), ([ent1, ent2, ...], [tag1, tag2, ...]), ...]\n    # as an output, we want to have tensor of entities and tensor of tags \n    sentences_batch, postags_batch = [], []\n    for _sent, _postags in batch:\n        n_pad = max_words_in_sentense - len(_sent)\n        if n_pad > 0:    \n            sentences_batch.append(_sent + [PAD_IDX for _ in range(n_pad)])\n            postags_batch.append(_postags + [cat2idx['X'] for _ in range(n_pad)])\n        else:\n            sentences_batch.append(_sent[:max_words_in_sentense])\n            postags_batch.append(_postags[:max_words_in_sentense])\n        len_sent = len(sentences_batch[-1])\n        len_postags = len(postags_batch[-1])\n        \n    sentences_batch = torch.tensor(sentences_batch, dtype=torch.int64)\n    postags_batch = torch.tensor(postags_batch, dtype=torch.int64)\n    # Remember, that if we want to perform many to many mapping with our network with recurrent units, \n    # we want pass first item from all sequences as first input, thus\n    # we want to have tensor with shape (max_size, ...., batch_size)\n    return sentences_batch.to(device), postags_batch.to(device)\nbatch_size = 128\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, drop_last = True\n)\nval_dataloader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch, drop_last = True\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:31:44.383285Z","iopub.execute_input":"2023-09-24T13:31:44.383834Z","iopub.status.idle":"2023-09-24T13:31:44.406299Z","shell.execute_reply.started":"2023-09-24T13:31:44.383782Z","shell.execute_reply":"2023-09-24T13:31:44.405140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just to check that all shapes are correct\n\nfor batch in train_dataloader:\n    inp, out = batch\n    print(inp.shape)\n    print(out.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:31:58.350725Z","iopub.execute_input":"2023-09-24T13:31:58.351433Z","iopub.status.idle":"2023-09-24T13:31:58.374511Z","shell.execute_reply.started":"2023-09-24T13:31:58.351395Z","shell.execute_reply":"2023-09-24T13:31:58.373606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the network\n\nFor the many-to-many or seq2seq netoworks, we want to have recurrent units in the network. This gives the ability for network to learn the hidden features and pass the knowledge from one token to other. \n\n### Embeddings\n\nFor embeddings you can use `nn.Embedding` for creating your own features or use pretrained embedding (like GloVe or FastText or Bert).\n\n### Recurrent\n\nFor processing sequences you can use recurrent units like `LSTM`.\n\n### Linear\n\nAdd simple nn.Linear. ~~This is basic stuff what do you want~~\n\n### Regularization\n\nRemeber to set up Dropout and Batch Normalization for regularization purposes.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass LSTMTagger(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=False)\n\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n\n        # The linear layer that maps from hidden state space to tag space\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        lstm_out = self.lstm(embeds)[0]\n        tag_space = self.hidden2tag(lstm_out)\n        tag_scores = tag_space\n        return tag_scores","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:01:45.007520Z","iopub.execute_input":"2023-09-24T14:01:45.008590Z","iopub.status.idle":"2023-09-24T14:01:45.016853Z","shell.execute_reply.started":"2023-09-24T14:01:45.008527Z","shell.execute_reply":"2023-09-24T14:01:45.015498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nAs for training you should take into account that the shape of your output and shape of the labels. Perform required transformations and use loss function that fits your task.\n\n> Do not forget about tqdm and logging, you want normal training not some unreadable ~~sht~~ logs. ","metadata":{}},{"cell_type":"code","source":"from tqdm.autonotebook import tqdm\n\ndef train_one_epoch(\n    model,\n    loader,\n    optimizer,\n    loss_fn,\n    epoch_num=-1\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch}: train\",\n        leave=True,\n    )\n    model.train()\n    train_loss = 0.0\n    total = 0\n    for i, batch in loop:\n        texts, labels = batch\n        \n        model.zero_grad()\n        \n        offsets = torch.zeros(1).to(device)\n        outputs = model(texts)\n        loss = loss_fn(\n            torch.reshape(outputs, (-1, 12)), \n            torch.reshape(labels, (-1, ))\n        )\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        loop.set_postfix({\"loss\": train_loss/(i*len(labels))})\n\ndef val_one_epoch(\n    model,\n    loader,\n    loss_fn,\n    score_fn,\n    epoch_num=-1,\n    best_so_far=0.0,\n    ckpt_path='best.pt'\n):\n    \n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch}: val\",\n        leave=True,\n    )\n    val_loss = 0.0\n    score = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, batch in loop:\n            texts, labels = batch\n            offsets = torch.zeros(1)\n            \n            outputs = model(texts)\n            labels_raveled = torch.reshape(labels, (-1, ))\n            outputs_raveled = torch.reshape(outputs, (-1, 12)) \n            loss = loss_fn(\n                outputs_raveled, \n                labels_raveled\n            )\n            predicted = torch.argmax(outputs_raveled.data, dim=1).to(device)\n            total += predicted.size(0)\n            correct += (predicted == labels_raveled).sum()\n            \n            score += score_fn(predicted, labels_raveled)\n            \n            loop.set_postfix({\"loss\": val_loss/total, \"acc\": correct / total, \"score\": score_fn(predicted, labels_raveled)})\n        \n        score /= len(loader)\n        if score > best_so_far:\n            torch.save(model.state_dict(), 'best_model.pt')\n            best_so_far = score\n\n    return best_so_far, val_loss","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:04:47.043375Z","iopub.execute_input":"2023-09-24T14:04:47.043900Z","iopub.status.idle":"2023-09-24T14:04:47.067406Z","shell.execute_reply.started":"2023-09-24T14:04:47.043860Z","shell.execute_reply":"2023-09-24T14:04:47.065856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:04:47.154102Z","iopub.execute_input":"2023-09-24T14:04:47.154396Z","iopub.status.idle":"2023-09-24T14:04:47.160395Z","shell.execute_reply.started":"2023-09-24T14:04:47.154369Z","shell.execute_reply":"2023-09-24T14:04:47.159454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim \nimport torchmetrics\n\nINPUT_DIM = len(train_dataset.vocab)\nOUTPUT_DIM = len(pos_tags)\n\nmodel = LSTMTagger( \n    embedding_dim=100, \n    hidden_dim=64, \n    vocab_size=len(train_vocab), \n    tagset_size=12\n).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=1, verbose=True)\nf1_score = torchmetrics.F1Score(task = \"multiclass\", num_classes = 12).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:04:47.330493Z","iopub.execute_input":"2023-09-24T14:04:47.331319Z","iopub.status.idle":"2023-09-24T14:04:47.386792Z","shell.execute_reply.started":"2023-09-24T14:04:47.331283Z","shell.execute_reply":"2023-09-24T14:04:47.385803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best = -float('inf')\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch_num=epoch)\n    best_so_far = val_one_epoch(model, val_dataloader, loss_fn, f1_score, epoch, best_so_far=best)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:04:47.618913Z","iopub.execute_input":"2023-09-24T14:04:47.619281Z","iopub.status.idle":"2023-09-24T14:05:58.952513Z","shell.execute_reply.started":"2023-09-24T14:04:47.619249Z","shell.execute_reply":"2023-09-24T14:05:58.950341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions\n\nWrite prediction. That's it. No more instructions, you already made it 3 times.","metadata":{}},{"cell_type":"code","source":"# you can use the same dataset class\ntest_dataset = PosTaggingDataset(test, cat2idx, train_vocab, if_train=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:11:23.914260Z","iopub.execute_input":"2023-09-24T14:11:23.915503Z","iopub.status.idle":"2023-09-24T14:11:24.155608Z","shell.execute_reply.started":"2023-09-24T14:11:23.915458Z","shell.execute_reply":"2023-09-24T14:11:24.154463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_batch(batch: list):\n    # Collate list of samples into tensor batch\n    # As an input we have list of pair from dataset:\n    # [([ent1, ent2, ...], [tag1, tag2, ...]), ([ent1, ent2, ...], [tag1, tag2, ...]), ...]\n    # as an output, we want to have tensor of entities and tensor of tags \n    sentences_batch, postags_batch = [], []\n    for _sent, _postags in batch:\n        n_pad = max_words_in_sentense - len(_sent)\n        if n_pad > 0:    \n            sentences_batch.append(_sent + [PAD_IDX for _ in range(n_pad)])\n            postags_batch.append(_postags + [cat2idx['X'] for _ in range(n_pad)])\n        else:\n            sentences_batch.append(_sent[:max_words_in_sentense])\n            postags_batch.append(_postags[:max_words_in_sentense])\n        len_sent = len(sentences_batch[-1])\n        len_postags = len(postags_batch[-1])\n        \n    sentences_batch = torch.tensor(sentences_batch, dtype=torch.int64)\n    postags_batch = torch.tensor(postags_batch, dtype=torch.int64)\n    # Remember, that if we want to perform many to many mapping with our network with recurrent units, \n    # we want pass first item from all sequences as first input, thus\n    # we want to have tensor with shape (max_size, ...., batch_size)\n    return sentences_batch.to(device), postags_batch.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\n\n# remebder that for training we can use pads but for testing we need to write \n# exact length of the sentence into the seubmission\ndef collate_batch(batch: list):\n    sentences_batch, sentences_lengths = [], []\n    for _sent in batch:\n        sentences_lengths.append(len(_sent))\n        n_pad = max_words_in_sentense - len(_sent)\n        sentences_batch.append(_sent + [PAD_IDX for _ in range(n_pad)])\n        \n    sentences_batch = torch.tensor(sentences_batch, dtype=torch.int64)\n    sentences_lengths = torch.tensor(sentences_lengths, dtype=torch.int64)\n    return sentences_batch.to(device), sentences_lengths.to(device)\n\ntest_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:16:09.755235Z","iopub.execute_input":"2023-09-24T14:16:09.755633Z","iopub.status.idle":"2023-09-24T14:16:09.763629Z","shell.execute_reply.started":"2023-09-24T14:16:09.755598Z","shell.execute_reply":"2023-09-24T14:16:09.762092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in test_dataloader:\n    inp, out = batch\n    print(inp.shape)\n    print(out)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:16:52.126462Z","iopub.execute_input":"2023-09-24T14:16:52.126879Z","iopub.status.idle":"2023-09-24T14:16:52.144304Z","shell.execute_reply.started":"2023-09-24T14:16:52.126849Z","shell.execute_reply":"2023-09-24T14:16:52.143335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(\n    model,\n    loader,\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Predictions\",\n        leave=True,\n    )\n    predictions = []\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, batch in loop:\n            text, text_len = batch\n            outputs = model(text)\n            \n            _, predicted = torch.max(outputs.data, 2)\n            for sent, sent_len in zip(predicted, text_len):\n                 predictions += sent[:sent_len]\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:22:28.187819Z","iopub.execute_input":"2023-09-24T14:22:28.188230Z","iopub.status.idle":"2023-09-24T14:22:28.196187Z","shell.execute_reply.started":"2023-09-24T14:22:28.188198Z","shell.execute_reply":"2023-09-24T14:22:28.195207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt = torch.load(\"/kaggle/working/best_model.pt\")\nmodel.load_state_dict(ckpt)\n\npredictions = predict(model, test_dataloader)\npredictions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:22:41.641808Z","iopub.execute_input":"2023-09-24T14:22:41.642265Z","iopub.status.idle":"2023-09-24T14:22:44.540896Z","shell.execute_reply.started":"2023-09-24T14:22:41.642231Z","shell.execute_reply":"2023-09-24T14:22:44.539884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labeled_preds = [idx2cat[int(pred)] for pred in predictions]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:24:47.622574Z","iopub.execute_input":"2023-09-24T14:24:47.622973Z","iopub.status.idle":"2023-09-24T14:24:51.143094Z","shell.execute_reply.started":"2023-09-24T14:24:47.622943Z","shell.execute_reply":"2023-09-24T14:24:51.142053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labeled_preds[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:25:06.336209Z","iopub.execute_input":"2023-09-24T14:25:06.336634Z","iopub.status.idle":"2023-09-24T14:25:06.344897Z","shell.execute_reply.started":"2023-09-24T14:25:06.336598Z","shell.execute_reply":"2023-09-24T14:25:06.343971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.Series(labeled_preds)\nresults.to_csv('submission.csv', index_label='id')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:26:00.265143Z","iopub.execute_input":"2023-09-24T14:26:00.265517Z","iopub.status.idle":"2023-09-24T14:26:01.005106Z","shell.execute_reply.started":"2023-09-24T14:26:00.265486Z","shell.execute_reply":"2023-09-24T14:26:01.004080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:26:05.172737Z","iopub.execute_input":"2023-09-24T14:26:05.173141Z","iopub.status.idle":"2023-09-24T14:26:05.188029Z","shell.execute_reply.started":"2023-09-24T14:26:05.173110Z","shell.execute_reply":"2023-09-24T14:26:05.185105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}